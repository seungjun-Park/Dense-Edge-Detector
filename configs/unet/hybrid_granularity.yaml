module:
  target: models.unet.UNet
  params:
    lr: 2e-5
    weight_decay: 0.0
    log_interval: 30
    loss_config:
      target: losses.l1lpips.L1LPIPS
      params:
        lpips_weight: 1.0
        l1_weight: 1.0
        ssim_weight: 1.0
        content_weight: 0.0
        granularity_weight: 1.0
        start_step: 800
        granularity_ckpt: './checkpoints/granularity/discriminator/best.ckpt'

    in_channels: 3
    embed_dim: 32
    out_channels: 1
    num_blocks: 2
    drop_path: 0.1
    activation: 'torch.nn.GELU'
    mode: 'bilinear'
    use_checkpoint: True
    scale_factors: [ 2, 2, 2 ]
    use_cond: True


data:
  target: datasets.util.DataModuleFromConfig
  params:
    num_workers: 4
    batch_size: 2
    wrap: True
    train:
      target: datasets.hybrid.HybridDataset
      params:
        anime_root: '/local_datasets/anime'
        biped_root: '/local_datasets/BIPED'
#        anime_root: 'D:/datasets/anime'
#        biped_root: 'D:/datasets/BIPED'
        train: True
        size: [512, 512]
        scale: [ 0.5, 1.0 ]

    validation:
      target: datasets.hybrid.HybridDataset
      params:
        anime_root: '/local_datasets/anime'
        biped_root: '/local_datasets/BIPED'
#        anime_root: 'D:/datasets/anime'
#        biped_root: 'D:/datasets/BIPED'
        train: False
        size: [512, 512]
        scale: [ 0.5, 1.0 ]

logger:
  target: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
  params:
    save_dir: './logs'
    name: 'unet'
    version: 'hybrid_granularity'


checkpoints:
  latest_checkpoints:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      dirpath: './checkpoints/unet/hybrid_granularity'
      filename: 'last'
      monitor: 'step'
      mode: 'max'
      save_top_k: 1

  best_checkpoint:
    target: pytorch_lightning.callbacks.ModelCheckpoint
    params:
      dirpath: './checkpoints/unet/hybrid_granularity'
      filename: 'best'
      monitor: 'valid/total_loss'
      mode: 'min'
      save_top_k: 1

trainer:
  accelerator: 'gpu'
  max_epochs: 200
  accumulate_grad_batches: 5
  # precision: 'bf16'