import importlib

import torch
import torch.nn as nn
import numpy as np
import torchvision.transforms as tf
import torch.fft
from collections import abc
from functools import partial
import math
import warnings
import cv2
from itertools import repeat
from typing import Union, List, Tuple

import multiprocessing as mp
from threading import Thread
from queue import Queue

from inspect import isfunction
from PIL import Image, ImageDraw, ImageFont
from typing import List, Union, Dict, Set


def ismap(x):
    if not isinstance(x, torch.Tensor):
        return False
    return (len(x.shape) == 4) and (x.shape[1] > 3)


def isimage(x):
    if not isinstance(x, torch.Tensor):
        return False
    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)


def exists(x):
    return x is not None


def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d


def mean_flat(tensor):
    """
    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86
    Take the mean over all non-batch dimensions.
    """
    return tensor.mean(dim=list(range(1, len(tensor.shape))))


def count_params(model, verbose=False):
    total_params = sum(p.numel() for p in model.parameters())
    if verbose:
        print(f"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params.")
    return total_params


def instantiate_from_config(config):
    if not "target" in config:
        if config == '__is_first_stage__':
            return None
        elif config == "__is_unconditional__":
            return None
        raise KeyError("Expected key `target` to instantiate.")
    if 'params' in config:
        module = get_obj_from_str(config["target"])(**config.get("params", dict()))
    else:
        module = get_obj_from_str(config['target'])()

    if 'ckpt' in config:
        ckpt = torch.load(config['ckpt'])['state_dict']
        module.load_state_dict(ckpt, strict=False)
    return module


def get_obj_from_str(string, reload=False):
    module, cls = string.rsplit(".", 1)
    if reload:
        module_imp = importlib.import_module(module)
        importlib.reload(module_imp)
    return getattr(importlib.import_module(module, package=None), cls)


def load_model_from_config(config, sd):
    model = instantiate_from_config(config)
    model.load_state_dict(sd, strict=False)
    model.cuda()
    model.eval()
    return model


def load_model(config, ckpt, gpu=True, eval_mode=True):
    if ckpt:
        print(f"Loading model from {ckpt}")
        pl_sd = torch.load(ckpt, map_location="cpu")
        global_step = pl_sd["global_step"]
    else:
        pl_sd = {"state_dict": None}
        global_step = None
    model = load_model_from_config(config.module,
                                   pl_sd["state_dict"])

    return model, global_step


True_set = ('yes', 'true', 't', 'y', '1')
False_set = ('no', 'false', 'f', 'n', '0')


def str2bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in True_set:
        return True
    elif v.lower() in False_set:
        return False
    elif v.lower() == 'none':
        return None
    else:
        raise ValueError("Boolean value expected.")


def to_flattened_numpy(x):
    return x.detach().cpu().numpy().reshape((-1,))


def from_flattened_numpy(x, shape):
    return torch.from_numpy(x.reshape(shape))


def _ntuple(n):
    def parse(x) -> Tuple:
        if isinstance(x, abc.Iterable) and not isinstance(x, str):
            return tuple(x)
        return tuple(repeat(x, n))

    return parse


to_1tuple = _ntuple(1)
to_2tuple = _ntuple(2)
to_3tuple = _ntuple(3)
to_4tuple = _ntuple(4)
to_ntuple = _ntuple


def multiply_integers(x) -> int:
    mul = 1
    if not isinstance(x, abc.Iterable):
        return x
    for i in x:
        mul *= i

    return int(mul)


def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (torch.Tensor, float, float, float, float) -> torch.Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)


def normalize_img(img):
    assert img.ndim == 3
    max_val = torch.max(img)
    min_val = torch.min(img)

    norm_x = (img - min_val) / (max_val - min_val)

    return norm_x


def zero_module(module):
    """
    Zero out the parameters of a module and return it.
    """
    for p in module.parameters():
        p.detach().zero_()
    return module


def to_rgb(inp: torch.Tensor, color_space: str):
    color_space = color_space.lower()
    if color_space == 'rgb':
        return inp
    elif color_space == 'bgr':
        color_space = cv2.COLOR_BGR2RGB
    elif color_space == 'rgba':
        color_space = cv2.COLOR_RGBA2RGB
    elif color_space == 'gray':
        color_space = cv2.COLOR_GRAY2RGB
    elif color_space == 'xyz':
        color_space = cv2.COLOR_XYZ2RGB
    elif color_space == 'ycrcb':
        color_space = cv2.COLOR_YCrCb2RGB
    elif color_space == 'hsv':
        color_space = cv2.COLOR_HSV2RGB
    elif color_space == 'lab':
        color_space = cv2.COLOR_LAB2RGB
    elif color_space == 'luv':
        color_space = cv2.COLOR_LUV2RGB
    elif color_space == 'hls':
        color_space = cv2.COLOR_HLS2RGB
    elif color_space == 'yuv':
        color_space = cv2.COLOR_YUV2RGB

    assert inp.ndim == 3

    inp = tf.ToPILImage()(inp)
    inp = np.array(inp)
    inp = cv2.cvtColor(inp, color_space)
    inp = tf.ToTensor()(inp)

    return inp


def adopt_weight(weight, global_step, threshold=0, value=0.):
    if global_step < threshold:
        weight = value
    return weight

def dictionary_filtering(keys: abc.Iterable, dictionary: Dict):
    keys = set(keys)
    return { k: v for k, v in dictionary.items() if k in keys }